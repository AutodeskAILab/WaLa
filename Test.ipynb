{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import open3d as o3d\n",
    "import os\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from src.dataset_utils import (\n",
    "    get_singleview_data,\n",
    "    get_multiview_data,\n",
    "    get_voxel_data_json,\n",
    "    get_image_transform_latent_model,\n",
    "    get_pointcloud_data,\n",
    "    get_mv_dm_data,\n",
    "    get_sv_dm_data,\n",
    "    get_sketch_data\n",
    ")\n",
    "from src.model_utils import Model\n",
    "from src.mvdream_utils import load_mvdream_model\n",
    "import argparse\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simplify_mesh(obj_path, target_num_faces=1000):\n",
    "    mesh = o3d.io.read_triangle_mesh(obj_path)\n",
    "    simplified_mesh = mesh.simplify_quadric_decimation(target_num_faces)\n",
    "    o3d.io.write_triangle_mesh(obj_path, simplified_mesh)\n",
    "\n",
    "\n",
    "def generate_3d_object(\n",
    "    model,\n",
    "    data,\n",
    "    data_idx,\n",
    "    scale,\n",
    "    diffusion_rescale_timestep,\n",
    "    save_dir=\"examples\",\n",
    "    output_format=\"obj\",\n",
    "    target_num_faces=None,\n",
    "    seed=42,\n",
    "):\n",
    "    # Set seed\n",
    "    seed_everything(seed, workers=True)\n",
    "\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.set_inference_fusion_params(scale, diffusion_rescale_timestep)\n",
    "    output_path = model.test_inference(\n",
    "        data, data_idx, save_dir=save_dir, output_format=output_format\n",
    "    )\n",
    "\n",
    "    if output_format == \"obj\" and target_num_faces:\n",
    "        simplify_mesh(output_path, target_num_faces=target_num_faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ADSKAILab/WaLa-SV-1B'\n",
    "images = ['examples/single_view/table.png']\n",
    "output_dir = 'examples'\n",
    "output_format = 'obj'\n",
    "target_num_faces = None\n",
    "scale = 1.8\n",
    "seed = 42\n",
    "diffusion_rescale_timestep = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/wala/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/utils.py:55: The loaded checkpoint was produced with Lightning v2.3.3, which is newer than your current Lightning version: v2.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DotDict' object has no attribute 'dataset_path'\n",
      "'DotDict' object has no attribute 'low_avg'\n",
      "'DotDict' object has no attribute 'low_avg'\n",
      "Low avg used : None high value: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ray/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/ray/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/ray/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/ray/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond_emb_dim: 1024\n",
      "Input resolution: 224\n",
      "Vocab size: N/A\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: examples/single_view/table.png\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa4ee1b534e4df89dc3ddae84990b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loading model\")\n",
    "\n",
    "model = Model.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "image_transform = get_image_transform_latent_model()\n",
    "\n",
    "for image_path in images:\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    data = get_singleview_data(\n",
    "        image_file=Path(image_path),\n",
    "        image_transform=image_transform,\n",
    "        device=model.device,\n",
    "        image_over_white=False,\n",
    "    )\n",
    "    data_idx = 0\n",
    "    save_dir = Path(output_dir) / Path(image_path).stem\n",
    "\n",
    "    model.set_inference_fusion_params(\n",
    "        scale, diffusion_rescale_timestep\n",
    "    )\n",
    "\n",
    "    generate_3d_object(\n",
    "        model,\n",
    "        data,\n",
    "        data_idx,\n",
    "        scale,\n",
    "        diffusion_rescale_timestep,\n",
    "        save_dir,\n",
    "        output_format,\n",
    "        target_num_faces,\n",
    "        seed,\n",
    "    )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: examples/single_view/table.png\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n",
      "'DotDict' object has no attribute 'use_multiple_views_inferences'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729e3b46bdf14acf9ed47b282ef94ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for image_path in images:\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    data = get_singleview_data(\n",
    "        image_file=Path(image_path),\n",
    "        image_transform=image_transform,\n",
    "        device=model.device,\n",
    "        image_over_white=False,\n",
    "    )\n",
    "    data_idx = 0\n",
    "    save_dir = Path(output_dir) / Path(image_path).stem\n",
    "\n",
    "    model.set_inference_fusion_params(\n",
    "        scale, diffusion_rescale_timestep\n",
    "    )\n",
    "\n",
    "    generate_3d_object(\n",
    "        model,\n",
    "        data,\n",
    "        data_idx,\n",
    "        scale,\n",
    "        diffusion_rescale_timestep,\n",
    "        save_dir,\n",
    "        output_format,\n",
    "        target_num_faces,\n",
    "        seed,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
